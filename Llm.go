/*
Go implementation of Claude logic from Anthropic API
*/

//TODO: ability to expose agents via  REST API to create agents as a service

package main

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"os"
	"strings"
)

type Agent struct {
	// Available Models
	// - claude-3-7-sonnet-20250219
	// - claude-3-5-sonnet-20241022 (claude-3-5-sonnet-latest)
	// - claude-3-5-haiku-20241022 (claude-3-5-haiku-latest)
	// - claude-3-opus-20240229 (claude-3-opus-latest)
	// - claude-3-sonnet-20240229
	// - claude-3-haiku-20240307
	Model          string
	Name           string
	ApiKey         string
	System         []Content
	ChatHistory    []Message
	Tools          []Tool
	CoreMemory     CoreMemory
	HeartbeatState bool
}

/*
Creates a new client to interact with the Agent.
The apiKey argument is optional; omitting it will attempt to obtain the ANTHROPIC_API_KEY from the user's environment variables.
Note that if the ANTHROPIC_API_KEY environment variable is not set, the function will return `false` as its second return value.
If the apiKey argument is included, the Agent will use the provided API key for making requests to the API.
*/
func NewAgent(model string, persona string, name string, apiKey ...string) (*Agent, bool) {
	var key string

	if len(apiKey) == 0 {
		userApiKey, ok := os.LookupEnv("ANTHROPIC_API_KEY")
		if !ok {
			return nil, false
		}
		key = userApiKey
	} else {
		key = apiKey[0]
	}

	agent := &Agent{
		Model:          model,
		ApiKey:         key,
		Name:           name,
		System:         []Content{{Type: "text", Text: SYSTEM_PROMPT}},
		ChatHistory:    make([]Message, 0),
		CoreMemory:     *NewCoreMemoryUnit(persona),
		HeartbeatState: false,
	}

	// providing the Agent system prompt as well as core memory data
	agent.System = append(agent.System, Content{Type: "text", Text: agent.CoreMemory.toString()})
	// providing necessary tools for memory management
	agent.Tools = append(agent.Tools, *agent.createCoreMemoryAppendTool(), *agent.createCoreMemoryReplaceTool())

	return agent, true
}

type AgentRequest struct {
	Model string `json:"model"`
	// the maximum number of tokens for the Agent to generate
	MaxTokens int    `json:"max_tokens,omitempty"`
	Tools     []Tool `json:"tools,omitempty"`
	/*
		Specify how the Agent should use the given tools. 'any' forces a tool to be used, 'tool' forces a specific tool to be used, and 'auto' does not enforce any tool usage.
		Follows schema: {"type" : "auto/any/tool"} --- If "type" is set to "tool", then you must include another key "name", which is the name of the tool to force the Agent to use.
		Parallel tool use can be disabled by adding `"disable_parallel_tool_use" = true`.
	*/
	ToolChoice map[string]string `json:"tool_choice,omitempty"`
	/*
		Defines the conversation history with the Agent
	*/
	Messages []Message `json:"messages"`
	// System prompt for Agent
	System []Content `json:"system,omitempty"`
	// Temperature value (0 - 1)
	Temperature float32 `json:"temperature,omitempty"`
}

// Defines the structure of a single message (i.e. either system or user prompt)
type Message struct {
	Role string `json:"role"` // 'user' or 'assistant'
	/*
			The text to pass into the Agent.
			Example:
			messages=[
		        {
		            "role": "user",
		            "content": "What's the weather like in San Francisco?"
		        },
		        {
		            "role": "assistant",
		            "content": [
		                {
		                    "type": "text",
		                    "text": "<thinking>I need to use get_weather, and the user wants SF, which is likely San Francisco, CA.</thinking>"
		                },
		                {
		                    "type": "tool_use",
		                    "id": "toolu_01A09q90qw90lq917835lq9",
		                    "name": "get_weather",
		                    "input": {"location": "San Francisco, CA", "unit": "celsius"}
		                }
		            ]
		        }
			]
	*/
	Content []Content `json:"content"`
}

// type Source struct {
// 	// E.g. "base64"
// 	Type string `json:"type"`
// 	// E.g. "image/jpeg"
// 	MediaType string `json:"media_type"`
// 	// E.g. "/9j/4AAQSkZJRg..."
// 	Data string `json:"data"`
// }

type Content struct {
	/*
		The type of content being provided back to the user/Agent.
		The type can be 'text', 'tool_use' (used when providing a Tool to the Agent for it to use), or 'tool_result' (used when passing the output of a Tool call back to the Agent)
	*/
	Type string `json:"type"`
	Text string `json:"text,omitempty"` // Agent response itself
	/*
		An ID value generated by the Agent's response when tool use is enabled.
		Use this value when passing the result of a tool call back to the Agent in another message
	*/
	ID   string `json:"id,omitempty"`
	Name string `json:"name,omitempty"`
	/*
		Map containing each parameter to pass into the function and the value to pass in for each parameter.
		Only applies for tool use.
	*/
	Input map[string]any `json:"input,omitempty"`
	/*
		The ID from the initial tool use call to the Agent.
		This ID is generated in the response after making an initial Agent call with tool use enabled.
		Use this key with 'role' = 'user' when passing the result from a tool call back to the Agent. Be sure to also include the 'content' key
	*/
	ToolUseID string `json:"tool_use_id,omitempty"`
	/*
		The output from the tool call.
		Use this key when passing the result from the tool call back to the Agent
	*/
	Content string `json:"content,omitempty"`
}

// type ResponseMessage struct {
// 	Content []Content `json:"content"`
// 	Role    string    `json:"role"`
// }

type AgentResponse struct {
	/*
		Contains multiple Content structs which correspond to different parts of the Agent's output.
		E.g. type "text" is the Agent's raw response, type "tool_use" is the model's tool call.
	*/
	Content []Content `json:"content"`
	ID      string    `json:"id"`
	Model   string    `json:"model,omitempty"`
	Role    string    `json:"role,omitempty"`
	//Type         string          `json:"type"`
	StopReason   string         `json:"stop_reason,omitempty"`
	StopSequence any            `json:"stop_sequence,omitempty"`
	Usage        map[string]int `json:"usage,omitempty"`
	//Message      ResponseMessage `json:"message"`
}

/*
Gets the Agent's string response from the user's request
*/
func (response AgentResponse) getOutput() string {
	var sb strings.Builder
	for _, content := range response.Content {
		if content.Type == "text" {
			sb.WriteString(content.Text)
			sb.WriteString("\n\n")

		} else if content.Type == "tool_use" {
			sb.WriteString(fmt.Sprintf("Tool Use ID: %s\n", content.ID))
			sb.WriteString(fmt.Sprintf("Tool name: %s\n", content.Name))
			for k, v := range content.Input {
				sb.WriteString(fmt.Sprintf("Input Parameter \"%s\" = \"%s\"", k, v))
			}
		}
	}
	return sb.String()
}

/*
Create a request for the Agent (custom messages parameter)
*/
func (llm Agent) NewAgentRequest(maxTokens int, temperature float32) *AgentRequest {
	return &AgentRequest{
		Model:       llm.Model,
		MaxTokens:   maxTokens,
		Tools:       llm.Tools,
		System:      llm.System,
		Messages:    llm.ChatHistory,
		Temperature: temperature,
	}
}

/*
Used to send a message back to the user
*/
func (llm *Agent) sendMessage(message string) string {
	fmt.Printf("Message from Agent:\n%s\n", message)
	llm.HeartbeatState = false
	return message
}

/*
Create a request that takes in a user prompt
*/
// func (llm Agent) NewPromptRequest(maxTokens int, system []Content, temperature float32, prompt string) *AgentRequest {
// 	return &AgentRequest{
// 		Model:     llm.Model,
// 		MaxTokens: maxTokens,
// 		//Tools:     tools,
// 		Messages: []Message{
// 			{Role: "user", Content: prompt},
// 		},
// 		System:      system,
// 		Temperature: temperature,
// 	}
// }

/*
Create a request that involves a user prompt as well as a partially filled response
*/
// func (llm Agent) NewPromptRequestWithResponseStarter(tools []Tool, maxTokens int, system []Content, temperature float32, prompt string, responseStarter string) *AgentRequest {
// 	return &AgentRequest{
// 		Model:     llm.Model,
// 		MaxTokens: maxTokens,
// 		Messages: []Message{
// 			{Role: "user", Content: prompt},
// 			{Role: "assistant", Content: responseStarter},
// 		},
// 		System:      system,
// 		Temperature: temperature,
// 	}
// }

/*
Function for calling an Agent via a prompt.
Returns the Response, the status code of the request, and error, if applicable
*/
func (llm Agent) call(reqData AgentRequest) (*AgentResponse, int, error) {
	// Extracting the request data
	reqBody, err := json.Marshal(reqData)
	if err != nil {
		fmt.Printf("Error marshaling request data: %v\n", err)
		return nil, -1, err
	}

	// Creating the request
	url := "https://api.anthropic.com/v1/messages"
	req, err := http.NewRequest("POST", url, bytes.NewBuffer(reqBody))

	if err != nil {
		fmt.Printf("Error creating request:%v\n", err)
		return nil, -1, err
	}

	// Setting request headers
	fmt.Println("Setting request headers")
	req.Header.Set("x-api-key", llm.ApiKey)
	req.Header.Set("anthropic-version", "2023-06-01")
	req.Header.Set("content-type", "application/json")

	fmt.Println("Making request")

	// Sending the response
	client := &http.Client{}
	res, err := client.Do(req)
	if err != nil {
		fmt.Printf("Error sending request %v\n", err)
		return nil, res.StatusCode, err
	}

	// Closing the response body once the function is finished executing
	defer res.Body.Close()

	fmt.Println("Reading response data")
	// Reading the response data
	resBody, err := io.ReadAll(res.Body)
	if err != nil {
		fmt.Printf("Error reading response %v\n", err)
		return nil, res.StatusCode, err
	}

	if res.StatusCode != http.StatusOK {
		fmt.Printf("Request failed with status code %d: %s\n", res.StatusCode, string(resBody))
	}

	fmt.Println("Storing response")
	// Storing the response into the Response struct
	var apiResp AgentResponse
	err = json.Unmarshal(resBody, &apiResp)
	if err != nil {
		fmt.Printf("Error parsing the response: %v\n", err)
		return nil, res.StatusCode, err
	}

	return &apiResp, res.StatusCode, nil

}

/*
Takes the output from the Agent and adds it to the conversation history.
*/
func (llm *Agent) addResponseToChatHistory(response AgentResponse) {
	// Updating the chat history with the last output from the Agent

	// A message can contain multiple Content structs
	messageToAppend := Message{Role: "assistant", Content: []Content{}}
	messageToAppend.Content = append(messageToAppend.Content, response.Content...)

	llm.ChatHistory = append(llm.ChatHistory, messageToAppend)
}

// func getWeather(args ...interface{}) interface{} {
// 	if len(args) == 0 {
// 		return "Error: No location provided"
// 	}

// 	location, ok := args[0].(string)
// 	if !ok {
// 		return "Error: Invalid location type"
// 	}

// 	switch location {
// 	case "San Francisco, CA":
// 		return float32(24.0)
// 	case "Boston, MA":
// 		return float32(10.0)
// 	case "New York City, NY":
// 		return float32(8.0)
// 	default:
// 		return float32(5.0)
// 	}
// }
